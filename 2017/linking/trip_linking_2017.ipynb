{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine separate walk-access/egress and transit trips into single transit trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Dataset_2 August 2017\\Trips\\5-Trip_rMove-v2.xlsx',\n",
    "             sheetname='5-Trip-rMove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the people whose mode changes at all between trips\n",
    "# Loop through each person in the survey\n",
    "\n",
    "# Ignore trips that are drop-off/pick-up. These might indicate a mode change (SOV to HOV 2 or 3+) but we\n",
    "# don't want them to be linked. \n",
    "\n",
    "# Max size of trip sets (don't try to link more than 4 trips)\n",
    "trip_set_max = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_ordered_list(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [ x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some trips have multiple modes listed - these need to be considered separately\n",
    "# single_mode_trips = trip[trip['mode_2'].isnull()]\n",
    "# multi_mode_trips = trip[-trip['mode_2'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Work with single mode trips first\n",
    "# df = multi_mode_trips\n",
    "\n",
    "# Get unique list of person\n",
    "uniquePersonIDs = df.groupby('personid').count().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the trip ID list by trip ID\n",
    "trip.sort('tripid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "daynum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GG</th>\n",
       "      <th>MJ HOME</th>\n",
       "      <th>MJ</th>\n",
       "      <th>NK</th>\n",
       "      <th>Time/speed</th>\n",
       "      <th>personid</th>\n",
       "      <th>recid</th>\n",
       "      <th>tripid</th>\n",
       "      <th>hhid</th>\n",
       "      <th>pernum</th>\n",
       "      <th>...</th>\n",
       "      <th>google_duration-SEC</th>\n",
       "      <th>trip_path_distance-mtr</th>\n",
       "      <th>user_merged</th>\n",
       "      <th>user_split</th>\n",
       "      <th>analyst_merged</th>\n",
       "      <th>analyst_split</th>\n",
       "      <th>flag_teleport</th>\n",
       "      <th>proxy_added_trip</th>\n",
       "      <th>nonproxy_derived_trip</th>\n",
       "      <th>child_trip_location_tripid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1710024801</td>\n",
       "      <td>D18250</td>\n",
       "      <td>1710024801001</td>\n",
       "      <td>17100248</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1710024801</td>\n",
       "      <td>D18251</td>\n",
       "      <td>1710024801002</td>\n",
       "      <td>17100248</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1085</td>\n",
       "      <td>2283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1710024801</td>\n",
       "      <td>D18252</td>\n",
       "      <td>1710024801003</td>\n",
       "      <td>17100248</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1106</td>\n",
       "      <td>8887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1710024801</td>\n",
       "      <td>D18253</td>\n",
       "      <td>1710024801004</td>\n",
       "      <td>17100248</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1228</td>\n",
       "      <td>1754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1710024801</td>\n",
       "      <td>D18254</td>\n",
       "      <td>1710024801005</td>\n",
       "      <td>17100248</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1116</td>\n",
       "      <td>6126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GG MJ HOME   MJ   NK Time/speed    personid   recid         tripid  \\\n",
       "41  NaN     NaN  NaN  NaN        NaN  1710024801  D18250  1710024801001   \n",
       "42  NaN     NaN  NaN  NaN        NaN  1710024801  D18251  1710024801002   \n",
       "43  NaN     NaN  NaN  NaN        NaN  1710024801  D18252  1710024801003   \n",
       "44  NaN     NaN  NaN  NaN        NaN  1710024801  D18253  1710024801004   \n",
       "45  NaN     NaN  NaN  NaN        NaN  1710024801  D18254  1710024801005   \n",
       "\n",
       "        hhid  pernum             ...              google_duration-SEC  \\\n",
       "41  17100248       1             ...                              180   \n",
       "42  17100248       1             ...                             1085   \n",
       "43  17100248       1             ...                             1106   \n",
       "44  17100248       1             ...                             1228   \n",
       "45  17100248       1             ...                             1116   \n",
       "\n",
       "    trip_path_distance-mtr  user_merged  user_split  analyst_merged  \\\n",
       "41                     224          NaN         NaN               0   \n",
       "42                    2283          NaN         NaN               0   \n",
       "43                    8887          NaN         NaN               0   \n",
       "44                    1754          NaN         NaN               0   \n",
       "45                    6126          NaN         NaN               0   \n",
       "\n",
       "    analyst_split flag_teleport  proxy_added_trip  nonproxy_derived_trip  \\\n",
       "41              0             0             False                    NaN   \n",
       "42              0             0             False                    NaN   \n",
       "43              0             0             False                    NaN   \n",
       "44              0             0             False                    NaN   \n",
       "45              0             0             False                    NaN   \n",
       "\n",
       "    child_trip_location_tripid  \n",
       "41                         NaN  \n",
       "42                         NaN  \n",
       "43                         NaN  \n",
       "44                         NaN  \n",
       "45                         NaN  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip[(trip['personid'] == person ) & (trip['daynum'] == daynum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1710024801]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[uniquePersonIDs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_trips = []\n",
    "person_counter = 0\n",
    "# Loop through each person day\n",
    "for person in uniquePersonIDs:\n",
    "# for person in [uniquePersonIDs[0]]:\n",
    "    \n",
    "    # loop through each person's travel day\n",
    "    travel_days = trip[trip['personid'] == person].groupby('daynum').count().index.values\n",
    "    for daynum in travel_days:\n",
    "        flag = 1\n",
    "        #print person_counter\n",
    "        \n",
    "        # Trips unique to a single travel day for one person\n",
    "        trip_subsample = trip[(trip['personid'] == person ) & (trip['daynum'] == daynum)]\n",
    "        \n",
    "        # Potential logic check: remove all persons that only drive for all trips\n",
    "        # Consider only transit/walk/bike trips for linking?\n",
    "            \n",
    "        # Loop through each person's trips\n",
    "        for row in xrange(0, len(trip_subsample)-1):\n",
    "            person_trip = trip_subsample.iloc[row]\n",
    "            next_pers_trip = trip_subsample.iloc[row+1]\n",
    "\n",
    "            # Are current and next trips linked?\n",
    "            # Ignore drop-off/pick-up. These might indicate a mode change (SOV to HOV 2 or 3+) but we don't want them to be linked. \n",
    "            # Also ignore purpose of mode transfer.\n",
    "            # Also ignore bus-bus trips\n",
    "            if (    (person_trip['mode_1'] <> next_pers_trip['mode_1']          # Include mode changes\n",
    "                or  (person_trip['mode_1'] == next_pers_trip['mode_1'] == 23)    # Include bus-to-bus transfer\n",
    "                or  (person_trip['mode_1'] == next_pers_trip['mode_1'] == 52))   # Include train-to-train transfer\n",
    "                and person_trip['a_dur'] <= 15                                                    \n",
    "                and (   person_trip['d_purp'] == next_pers_trip['d_purp']     # Trip purp must be the same\n",
    "                     or person_trip['d_purp'] == 60)                     # or purp listed as \"mode change\"\n",
    "                and person_trip['d_purp'] <> 9                           # Exclude drop-off/pick-up trips\n",
    "#                 or person_trip['d_purp'] == 15 and next_pers_trip['mode_1'] <> 15  # Include all mode changes except planes\n",
    "                ):\n",
    "                # If this looks unlinked, flag it\n",
    "                problem_trips.append([\"%05d\" % (person_counter,) + \"%02d\" % (flag,), person_trip['tripid']])\n",
    "                problem_trips.append([\"%05d\" % (person_counter,) + \"%02d\" % (flag,), next_pers_trip['tripid']])\n",
    "\n",
    "                # Is this trip part of an existing linked trip pair or a new trip pair?\n",
    "                # Is the activity duration longer 30 minutes? Then it's probably a separate commute trip\n",
    "                # I moved this to the outer loop - originally it was inside the above if statement...\n",
    "                # The linked ID is no longer sequential but it is at least unique\n",
    "            if next_pers_trip['a_dur'] > 30 or next_pers_trip['dest_name'] == 'HOME':\n",
    "                flag += 1\n",
    "\n",
    "        person_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_trips_df = pd.DataFrame(problem_trips,columns=['linked_flag', 'tripid'])\n",
    "merged_trips = pd.merge(left=trip,right=problem_trips_df,on='tripid',left_index=True,how='outer')\n",
    "merged_trips.drop_duplicates(inplace=True) # Remove duplicates\n",
    "merged_trips.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate unlinked trips\n",
    "unlinked_trips = merged_trips.query(\"linked_flag <> 0\")\n",
    "\n",
    "# List of all linked trip sets and the number of records in each\n",
    "unlinked_sets = unlinked_trips.groupby('linked_flag').count()['recid']\n",
    "\n",
    "# Summary statistics on linked trip sets - gives us an idea of how well we identified linked trips\n",
    "setsize = {}\n",
    "for idx in list(unlinked_sets.index):\n",
    "     # Examine each set of linked trips\n",
    "     trip_set = unlinked_trips[unlinked_trips['linked_flag'] == idx]\n",
    "     setsize[idx] = len(trip_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "1      \n",
       "2   525\n",
       "3    91\n",
       "4    46\n",
       "5     7\n",
       "6     2\n",
       "7     1\n",
       "8     1\n",
       "10    1\n",
       "13    1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find distribution of set sizes\n",
    "df_setsize = pd.DataFrame([setsize.keys(), setsize.values()]).T\n",
    "df_setsize.index = df_setsize[0]    # Set index equal to the set ID\n",
    "\n",
    "setsize_dist = df_setsize.groupby(1).count()   # Distribution of set size\n",
    "setsize_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_excel() got an unexpected keyword argument 'cols'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-af9a98306d0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;31m# Trip file with ALL unlinked files removed and new linked trips added (reording cols to match original trip file order)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m \u001b[0mtrip_with_linked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Linked Trips Combined\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrip_unlinked_removed_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'combined_modes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'num_trips_linked'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;31m# Trips with ALL unlinked trips removed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: to_excel() got an unexpected keyword argument 'cols'"
     ]
    }
   ],
   "source": [
    "# Distribution shows that most (90%) of sets are 2 or 3 trips only. Let's automatically join these only and do the others manually. \n",
    "# Discard sets with more than 3 trips because these are too unusual\n",
    "#linked_list = linked_list[linked_list <= 3]\n",
    "\n",
    "unlinked_trips_df = pd.DataFrame(unlinked_trips)\n",
    "unlinked_trips_df.index = unlinked_trips.linked_flag    # Change index to work with the flag\n",
    "\n",
    "# Get mode combination for each set\n",
    "unlinked_trips_df['mode_1'] = unlinked_trips_df['mode_1'].astype(\"int64\")   # Convert from float to int first\n",
    "unlinked_trips_df['mode_1'] = pd.DataFrame(unlinked_trips_df['mode_1'].astype(\"str\"))     # Convert to string\n",
    "# Create new column with concatentation of modes\n",
    "unlinked_trips_df['combined_modes'] = unlinked_trips_df.groupby('linked_flag').apply(lambda x: '-'.join(x['mode_1']))\n",
    "\n",
    "# We could also concatenate other fields in this way...\n",
    "#unlinked_trips_df['driver'] = unlinked_trips_df['driver'].astype(\"int64\")   # Convert from float to int first\n",
    "#unlinked_trips_df['driver'] = pd.DataFrame(unlinked_trips_df['driver'].astype(\"str\"))     # Convert to string\n",
    "#unlinked_trips_df['linked_driver'] = unlinked_trips_df.groupby('linked_flag').apply(lambda x: '-'.join(x['driver']))\n",
    "\n",
    "# Filter out sets with more than 4 unlinked trip and flag them for manual inspection\n",
    "# The name \"..._max4\" is poorly titled. The max set size is now flexible so that greater or fewer \n",
    "unlinked_trips_max4 = unlinked_trips_df[unlinked_trips_df['combined_modes'].str.count('-') < trip_set_max]\n",
    "\n",
    "# Want the sum of all trips in a set for these values\n",
    "sum_fields = ['trip_path_distance', 'google_duration', 'reported_duration']\n",
    "\n",
    "# Want the max of all trips in a set for these values (to capture any instance of use)\n",
    "# This captures any instance of use in the trip set and assumes only 1 instance per set.\n",
    "# This is sort of okay since we only link 4 trips and it's unlinkely many of these fields will have multiple\n",
    "# instances, but it should be more methodical in the future. \n",
    "max_fields = ['taxi_type', 'taxi_pay', 'driver', 'toll', 'toll_pay','park_ride_area_start',\n",
    "              'change_vehicles', 'park','park_pay', 'mode_acc', 'mode_egr']\n",
    "\n",
    "# Convert to consistent type - float 64\n",
    "for field in sum_fields:\n",
    "    unlinked_trips_max4[field] = unlinked_trips_max4[field].astype(\"float64\")\n",
    "\n",
    "# Convert transitline data into integer\n",
    "for field in ['transit_line_' + str(x) for x in xrange(1,5)]:\n",
    "    unlinked_trips_max4[field] = unlinked_trips_max4[field].astype(\"int\")\n",
    "\n",
    "# Get the sums and max values of trips grouped by each person's set\n",
    "sums = unlinked_trips_max4.groupby('linked_flag').sum()\n",
    "maxes = unlinked_trips_max4.groupby('linked_flag').max()\n",
    "\n",
    "# Now we want to squish those unlinked trips together!\n",
    "# The \"primary trip\" will inherit characeristics of associated trips\n",
    "# Return list of primary trips and max distance for each set\n",
    "#primary_trips = linked_trips_df.groupby('linked_flag').max()[['tripID','gdist']]\n",
    "\n",
    "# change index to be trip ID because this is the number we ultimately want\n",
    "df = pd.DataFrame(unlinked_trips_max4)\n",
    "df.index = unlinked_trips_max4['tripid']\n",
    "# Find the trip ID of the longest trip in each set\n",
    "primary_trips = pd.DataFrame(df.groupby('linked_flag')['trip_path_distance'].agg(lambda x: x.idxmax()))\n",
    "#unlinked_trips_max4.groupby('linked_flag')\n",
    "\n",
    "# Select only the primary trip from each set\n",
    "primary_trips_df = unlinked_trips_max4[df['tripid'].isin(primary_trips['trip_path_distance'])]\n",
    "primary_trips_df.index = primary_trips_df.linked_flag   # Reset index to trip set ID\n",
    "\n",
    "# Change primary trip start time to time of first in linked trip set\n",
    "for field in ['depart_time_mam', 'depart_time_hhmm', 'o_purp', 'origin_name', 'origin_lat', 'origin_lng']:\n",
    "    # Save the original data in a new column\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    primary_trips_df.loc[:,field] = df.groupby('linked_flag').apply(lambda x: x[field].iloc[0])\n",
    "\n",
    "# Change primary trip start time to time of last in linked trip set\n",
    "# Change primary purpose and activity duration to that of the last trip in the set\n",
    "for field in ['arrival_time_mam', 'arrival_time_hhmm', 'a_dur', 'd_purp','dest_lat', 'dest_lng']:\n",
    "    # Save the original data in a new column\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    primary_trips_df.loc[:,field] = df.groupby('linked_flag').apply(lambda x: x[field].iloc[-1])\n",
    "    \n",
    "for field in sum_fields:\n",
    "    # Save original primary trip info in a new column appened with \"_original\"\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    # Replace the primary trip fields with summed data\n",
    "    primary_trips_df.loc[:,field] = sums[field]\n",
    "\n",
    "for field in max_fields:\n",
    "    # Save original primary trip info in a new column appened with \"_original\"\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    # Replace the primary trip fields with summed data\n",
    "    primary_trips_df.loc[:,field] = maxes[field]\n",
    "\n",
    "#df_min_stop_time = df_stop_times.sort('stop_sequence', ascending=True).groupby('trip_id', as_index=False).first()\n",
    "##need min stop time\n",
    "#df_trips = pd.merge(left = df_trips, right = df_min_stop_time, on=['trip_id'])\n",
    "\n",
    "## Save transitline data into primary trip record\n",
    "#tr1 = pd.DataFrame(df.groupby('linked_flag')[['transitline1']].agg(lambda x: x.tolist()))\n",
    "## Create new column to store unique transitline trips\n",
    "#for each in ['transitline' + str(x) for x in xrange(1,5)]:\n",
    "#    primary_trips_df[each + '_list'] = \"\"\n",
    "#tr2 = pd.DataFrame(df.groupby('linked_flag')['transitline2'].agg(lambda x: x.tolist()))\n",
    "\n",
    "# this returns greater than zero values for a single row - a single list of a list\n",
    "\n",
    "# Collect all transitline1 values for a set in a single array\n",
    "tr1 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_1']].agg(lambda x: x.tolist()))\n",
    "tr2 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_2']].agg(lambda x: x.tolist()))\n",
    "tr3 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_3']].agg(lambda x: x.tolist()))\n",
    "tr4 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_4']].agg(lambda x: x.tolist()))\n",
    "ts1 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_1']].agg(lambda x: x.tolist()))\n",
    "ts2 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_2']].agg(lambda x: x.tolist()))\n",
    "ts3 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_3']].agg(lambda x: x.tolist()))\n",
    "ts4 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_4']].agg(lambda x: x.tolist()))\n",
    "\n",
    "# Add together all the transitline values (1 through 4)\n",
    "combined_transitlines = pd.DataFrame(tr1['transit_line_1'] + tr2['transit_line_2'] + tr3['transit_line_3'] + tr4['transit_line_4'])\n",
    "combined_transitsys = pd.DataFrame(ts1['transit_system_1'] + ts2['transit_system_2'] + ts3['transit_system_3'] + ts4['transit_system_4'])\n",
    "#combined_transitlines[0].iloc[0]\n",
    "\n",
    "combined_transitlines[\"tr1\"] = \"\"\n",
    "combined_transitlines[\"tr2\"] = \"\"\n",
    "combined_transitlines[\"tr3\"] = \"\"\n",
    "combined_transitlines[\"tr4\"] = \"\"\n",
    "combined_transitsys[\"ts1\"] = \"\"\n",
    "combined_transitsys[\"ts2\"] = \"\"\n",
    "combined_transitsys[\"ts3\"] = \"\"\n",
    "combined_transitsys[\"ts4\"] = \"\"\n",
    "\n",
    "# Number of columns for transit lines or transit systems (4 in 2014 survey design)\n",
    "num_transitlines = 4\n",
    "num_transys = 4\n",
    "\n",
    "for row in xrange(0, len(combined_transitlines)):\n",
    "    # Add all unlinked trips' transitline data into a list\n",
    "    combined_transitlines[0].iloc[row] = unique_ordered_list(combined_transitlines[0].iloc[row])  #[0] selects df column\n",
    "    combined_transitsys[0].iloc[row] = unique_ordered_list(combined_transitsys[0].iloc[row])  #[0] selects df column\n",
    "    # Remove zeros that might be at beginning of the list\n",
    "    combined_transitlines[0].iloc[row] = [x for x in combined_transitlines[0].iloc[row] if x != 0]\n",
    "    combined_transitsys[0].iloc[row] = [x for x in combined_transitsys[0].iloc[row] if x != 0]\n",
    "    # But we want to pad the rest with zeros for consistent array shape\n",
    "    combined_transitlines[0].iloc[row] = np.pad(combined_transitlines[0].iloc[row],\n",
    "                                                (0,num_transitlines-len(combined_transitlines[0].iloc[row])),\n",
    "                                                mode='constant')\n",
    "    combined_transitsys[0].iloc[row] = np.pad(combined_transitsys[0].iloc[row],\n",
    "                                                (0,num_transitlines-len(combined_transitsys[0].iloc[row])),\n",
    "                                                mode='constant')\n",
    "\n",
    "    for i in xrange(4):\n",
    "        combined_transitlines[\"tr\" + str(i + 1)].iloc[row] = combined_transitlines[0].iloc[row][i]\n",
    "        combined_transitsys[\"ts\" + str(i + 1)].iloc[row] = combined_transitsys[0].iloc[row][i]\n",
    "\n",
    "# Add the transitline values to the primary trip record\n",
    "for i in xrange(1,5):\n",
    "    primary_trips_df['transit_line_' + str(i)] = combined_transitlines['tr' + str(i)]\n",
    "    primary_trips_df['transit_system_' + str(i)] = combined_transitsys['ts' + str(i)]\n",
    "\n",
    "\n",
    "# Trips with all unlinked trips removed\n",
    "# note the \"-trip\" call to grab inverse of selection, so we're getting all survey trips NOT in unlinked_trips_df\n",
    "trip_unlinked_removed_all = trip[-trip['tripid'].isin(unlinked_trips_df.tripid)]   # ALL unlinked trips removed\n",
    "\n",
    "\n",
    "# # Okay now we want to filter out some bad linked trips and just import the unlinked trip\n",
    "# # Do this before we add the linked trips onto the main file\n",
    "# home2home = primary_trips_df.query(\"place_end == 'HOME' and place_start == 'HOME'\")\n",
    "\n",
    "# # List of bad links is the manually ID'ed linked_flag value of trips that look incorrect.\n",
    "# # Remove these from the auto-linked trip and keep in the unlinked trip file\\\n",
    "# #bad_links = pd.read_csv(bad_trips)\n",
    "# with open(bad_trips, 'r') as f:\n",
    "#     bad_trip_list = []\n",
    "#     for item in f:\n",
    "#         bad_trip_list.append(item[:-1])\n",
    "\n",
    "# bad_trip_df = primary_trips_df[primary_trips_df['linked_flag'].isin(bad_trip_list)]\n",
    "# # Append the home2home trips on the bad_trip_df\n",
    "# bad_trip_df = bad_trip_df.append(home2home)\n",
    "\n",
    "# # Remove bad trips from combined trip file\n",
    "# primary_trips_df = primary_trips_df[-primary_trips_df['tripID'].isin(bad_trip_df.tripID)]\n",
    "# primary_trips_df['linked_flag'] = primary_trips_df.index\n",
    "\n",
    "# # Add unlinked trips back in to unlinked file\n",
    "# #unlinked_trips_df = unlinked_trips_df.append(unlinked_trips_df[unlinked_trips_df['linked_flag'].isin(bad_trip_df.linked_flag)])     \n",
    "\n",
    "# Trips with all linked trips added (and unlinked trips removed)\n",
    "trip_with_linked = pd.concat([trip_unlinked_removed_all,primary_trips_df])\n",
    "\n",
    "# List of still unlinked trips - these still need to be addressed\n",
    "# unprocessed_unlinked_trips = unlinked_trips_df[unlinked_trips_df['combined_modes'].str.count('-') >= trip_set_max]\n",
    "# unprocessed_unlinked_trips = unprocessed_unlinked_trips.append(unlinked_trips_df[unlinked_trips_df['linked_flag'].isin(bad_trip_df.linked_flag)])    \n",
    "\n",
    "# # Distribution of combined trip modes\n",
    "# a = primary_trips_df.groupby('combined_modes').count()['recordID']\n",
    "\n",
    "# # Add the count of unlinked trips in each linked trip \n",
    "# trip_with_linked['num_trips_linked'] = df_setsize[1]\n",
    "#trip_with_linked['num_trips_linked'].fillna(0)\n",
    "\n",
    "# Reorder columns to match original trip file\n",
    "#trip_with_linked.columns(trip_unlinked_removed_all.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Send to excel\n",
    "writer = pd.ExcelWriter('trip_linking_NEW.xlsx')\n",
    "\n",
    "# Trip file with ALL unlinked files removed and new linked trips added (reording cols to match original trip file order)\n",
    "trip_with_linked.to_excel(writer, \"Linked Trips Combined\", cols=list(trip_unlinked_removed_all.columns) + ['combined_modes', 'num_trips_linked'])\n",
    "\n",
    "# Trips with ALL unlinked trips removed\n",
    "trip_unlinked_removed_all.to_excel(writer, 'All Unlinked Trips Removed')\n",
    "\n",
    "# Linked Trips only\n",
    "# Join with regular trip file data\n",
    "primary_trips_df.to_excel(writer, 'Linked Trips Only')\n",
    "\n",
    "# Unlinked Trips only\n",
    "unlinked_trips_df.to_excel(writer, 'Unlinked Trips Only')\n",
    "\n",
    "# List of unprocessed unlinked trips\n",
    "unprocessed_unlinked_trips.to_excel(writer, \"Unprocessed Unlinked Trips\")\n",
    "\n",
    "# Unlinked trips that need to be edited by hand\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
