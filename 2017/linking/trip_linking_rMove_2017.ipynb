{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine separate walk-access/egress and transit trips into single transit trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = pd.read_excel(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Dataset_2 August 2017\\Trips\\5-Trip_rMove-v4.xlsx',\n",
    "             sheetname='5-Trip-rMove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First find the people whose mode changes at all between trips\n",
    "# Loop through each person in the survey\n",
    "\n",
    "# Ignore trips that are drop-off/pick-up. These might indicate a mode change (SOV to HOV 2 or 3+) but we\n",
    "# don't want them to be linked. \n",
    "\n",
    "# Max size of trip sets (don't try to link more than 4 trips)\n",
    "trip_set_max = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_ordered_list(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [ x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Some trips have multiple modes listed - these need to be considered separately\n",
    "# single_mode_trips = trip[trip['mode_2'].isnull()]\n",
    "# multi_mode_trips = trip[-trip['mode_2'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Work with single mode trips first\n",
    "# df = multi_mode_trips\n",
    "\n",
    "# Get unique list of person\n",
    "uniquePersonIDs = trip.groupby('personid').count().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the trip ID list by trip ID\n",
    "trip.sort('tripid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daynum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1710005901]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[uniquePersonIDs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_trips = []\n",
    "person_counter = 0\n",
    "# Loop through each person day\n",
    "for person in uniquePersonIDs:\n",
    "# for person in [uniquePersonIDs[0]]:\n",
    "    \n",
    "    # loop through each person's travel day\n",
    "    travel_days = trip[trip['personid'] == person].groupby('daynum').count().index.values\n",
    "    for daynum in travel_days:\n",
    "        flag = 1\n",
    "        #print person_counter\n",
    "        \n",
    "        # Trips unique to a single travel day for one person\n",
    "        trip_subsample = trip[(trip['personid'] == person ) & (trip['daynum'] == daynum)]\n",
    "        \n",
    "        # Potential logic check: remove all persons that only drive for all trips\n",
    "        # Consider only transit/walk/bike trips for linking?\n",
    "            \n",
    "        # Loop through each person's trips\n",
    "        for row in xrange(0, len(trip_subsample)-1):\n",
    "            person_trip = trip_subsample.iloc[row]\n",
    "            next_pers_trip = trip_subsample.iloc[row+1]\n",
    "\n",
    "            # Are current and next trips linked?\n",
    "            # Ignore drop-off/pick-up. These might indicate a mode change (SOV to HOV 2 or 3+) but we don't want them to be linked. \n",
    "            # Also ignore purpose of mode transfer.\n",
    "            # Also ignore bus-bus trips\n",
    "            if (    (person_trip['mode_1'] <> next_pers_trip['mode_1']          # Include mode changes, otherwise likely same mode is take for return leg of tour\n",
    "                or  (person_trip['mode_1'] == next_pers_trip['mode_1'] == 23)    # Include bus-to-bus transfer\n",
    "                or  (person_trip['mode_1'] == next_pers_trip['mode_1'] == 52))   # Include train-to-train transfer\n",
    "                and person_trip['a_dur'] <= 15    # Transfer must be under 15 minutes                                                  \n",
    "                and (   person_trip['d_purp'] == next_pers_trip['d_purp']     # Trip purp must be the same\n",
    "                     or person_trip['d_purp'] == 60)                     # or purp listed as \"mode change\"\n",
    "                and person_trip['d_purp'] <> 9                           # Exclude drop-off/pick-up trips\n",
    "                or person_trip['d_purp'] == 15 and next_pers_trip['mode_1'] <> 31  # Include all mode changes except planes\n",
    "#                 or person_trip['Loc_type'] in ['P&R','PARKING','TRANSIT']\n",
    "                ):\n",
    "                # If this looks unlinked, flag it\n",
    "                problem_trips.append([\"%05d\" % (person_counter,) + \"%02d\" % (flag,), person_trip['tripid']])\n",
    "                problem_trips.append([\"%05d\" % (person_counter,) + \"%02d\" % (flag,), next_pers_trip['tripid']])\n",
    "\n",
    "                # Is this trip part of an existing linked trip pair or a new trip pair?\n",
    "                # Is the activity duration longer 30 minutes? Then it's probably a separate commute trip\n",
    "                # I moved this to the outer loop - originally it was inside the above if statement...\n",
    "                # The linked ID is no longer sequential but it is at least unique\n",
    "            if next_pers_trip['a_dur'] > 30 or next_pers_trip['dest_name'] == 'HOME' or next_pers_trip['dest_name'] == 'WORK':\n",
    "                flag += 1\n",
    "\n",
    "        person_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_trips_df = pd.DataFrame(problem_trips,columns=['linked_flag', 'tripid'])\n",
    "merged_trips = pd.merge(left=trip,right=problem_trips_df,on='tripid',left_index=True,how='outer')\n",
    "merged_trips.drop_duplicates(inplace=True) # Remove duplicates\n",
    "merged_trips.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolate unlinked trips\n",
    "unlinked_trips = merged_trips.query(\"linked_flag <> 0\")\n",
    "\n",
    "# List of all linked trip sets and the number of records in each\n",
    "unlinked_sets = unlinked_trips.groupby('linked_flag').count()['recid']\n",
    "\n",
    "# Summary statistics on linked trip sets - gives us an idea of how well we identified linked trips\n",
    "setsize = {}\n",
    "for idx in list(unlinked_sets.index):\n",
    "     # Examine each set of linked trips\n",
    "     trip_set = unlinked_trips[unlinked_trips['linked_flag'] == idx]\n",
    "     setsize[idx] = len(trip_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "1       \n",
       "2   1183\n",
       "3    213\n",
       "4     73\n",
       "5     16\n",
       "6      2\n",
       "7      3\n",
       "10     1\n",
       "13     1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find distribution of set sizes\n",
    "df_setsize = pd.DataFrame([setsize.keys(), setsize.values()]).T\n",
    "df_setsize.index = df_setsize[0]    # Set index equal to the set ID\n",
    "\n",
    "setsize_dist = df_setsize.groupby(1).count()   # Distribution of set size\n",
    "setsize_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribution shows that most (90%) of sets are 2 or 3 trips only. Let's automatically join these only and do the others manually. \n",
    "# Discard sets with more than 3 trips because these are too unusual\n",
    "# linked_list = linked_list[linked_list <= 3]\n",
    "\n",
    "unlinked_trips_df = pd.DataFrame(unlinked_trips)\n",
    "unlinked_trips_df.index = unlinked_trips.linked_flag    # Change index to work with trip sets with same flag id\n",
    "\n",
    "# Get mode combination for each set\n",
    "unlinked_trips_df['mode_1'] = unlinked_trips_df['mode_1'].astype(\"int64\")   # Convert from float to int first\n",
    "unlinked_trips_df['mode_1'] = pd.DataFrame(unlinked_trips_df['mode_1'].astype(\"str\"))     # Convert to string\n",
    "# Create new column with concatentation of modes\n",
    "unlinked_trips_df['combined_modes'] = unlinked_trips_df.groupby('linked_flag').apply(lambda x: '-'.join(x['mode_1']))\n",
    "\n",
    "# We could also concatenate other fields in this way...\n",
    "#unlinked_trips_df['driver'] = unlinked_trips_df['driver'].astype(\"int64\")   # Convert from float to int first\n",
    "#unlinked_trips_df['driver'] = pd.DataFrame(unlinked_trips_df['driver'].astype(\"str\"))     # Convert to string\n",
    "#unlinked_trips_df['linked_driver'] = unlinked_trips_df.groupby('linked_flag').apply(lambda x: '-'.join(x['driver']))\n",
    "\n",
    "# Filter out sets with more than 4 unlinked trip and flag them for manual inspection\n",
    "# The name \"..._max4\" is poorly titled. The max set size is now flexible so that greater or fewer \n",
    "unlinked_trips_max4 = unlinked_trips_df[unlinked_trips_df['combined_modes'].str.count('-') < trip_set_max]\n",
    "\n",
    "# Want the sum of all trips in a set for these values\n",
    "sum_fields = ['trip_path_distance', 'google_duration', 'reported_duration',\n",
    "             'bus_pay','ferry_pay','rail_pay','air_pay']\n",
    "\n",
    "# Want the max of all trips in a set for these values (to capture any instance of use)\n",
    "# This captures any instance of use in the trip set and assumes only 1 instance per set.\n",
    "# This is sort of okay since we only link up to 3 trips and it's unlinkely many of these fields will have multiple\n",
    "# instances, but it should be more methodical in the future. \n",
    "max_fields = ['taxi_type', 'taxi_pay', 'driver', 'toll', 'toll_pay',\n",
    "              'park_ride_area_start','park_ride_area_end', 'park_ride_lot_start',\n",
    "              'park_ride_lot_end', 'bus_type','bus_cost_dk',\n",
    "              'ferry_type','ferry_cost_dk','rail_type','rail_cost_dk',\n",
    "              'air_type','airfare_cost_dk',\n",
    "              'change_vehicles', 'park','park_type','park_pay', 'mode_acc', 'mode_egr', ]\n",
    "\n",
    "# Convert to consistent type - float 64\n",
    "for field in sum_fields:\n",
    "    unlinked_trips_max4[field] = unlinked_trips_max4[field].astype(\"float64\")\n",
    "\n",
    "# Convert transitline data into integer\n",
    "for field in ['transit_line_' + str(x) for x in xrange(1,5)]:\n",
    "    unlinked_trips_max4[field] = unlinked_trips_max4[field].astype(\"int\")\n",
    "\n",
    "# Get the sums and max values of trips grouped by each person's set\n",
    "sums = unlinked_trips_max4.groupby('linked_flag').sum()\n",
    "maxes = unlinked_trips_max4.groupby('linked_flag').max()\n",
    "\n",
    "# Now we want to squish those unlinked trips together!\n",
    "# The \"primary trip\" will inherit characeristics of associated trips\n",
    "# Return list of primary trips and max distance for each set\n",
    "#primary_trips = linked_trips_df.groupby('linked_flag').max()[['tripID','gdist']]\n",
    "\n",
    "# change index to be trip ID because this is the number we ultimately want\n",
    "df = pd.DataFrame(unlinked_trips_max4)\n",
    "df.index = unlinked_trips_max4['tripid']\n",
    "# Find the trip ID of the longest trip in each set\n",
    "primary_trips = pd.DataFrame(df.groupby('linked_flag')['trip_path_distance'].agg(lambda x: x.idxmax()))\n",
    "#unlinked_trips_max4.groupby('linked_flag')\n",
    "\n",
    "# Select only the primary trip from each set\n",
    "primary_trips_df = unlinked_trips_max4[df['tripid'].isin(primary_trips['trip_path_distance'])]\n",
    "primary_trips_df.index = primary_trips_df.linked_flag   # Reset index to trip set ID\n",
    "\n",
    "# Change primary trip start time to time of first in linked trip set\n",
    "for field in ['depart_time_mam', 'depart_time_hhmm','depart_time_timestamp','o_purp',\n",
    "              'origin_name','origin_address', 'origin_lat', 'origin_lng']:\n",
    "    # Save the original data in a new column\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    primary_trips_df.loc[:,field] = df.groupby('linked_flag').apply(lambda x: x[field].iloc[0])\n",
    "\n",
    "# Change primary trip start time to time of last in linked trip set\n",
    "# Change primary purpose and activity duration to that of the last trip in the set\n",
    "for field in ['arrival_time_mam', 'arrival_time_hhmm','arrival_time_timestamp',\n",
    "              'a_dur', 'd_purp','dest_lat', 'dest_lng','dest_name','dest_address']:\n",
    "    # Save the original data in a new column\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    primary_trips_df.loc[:,field] = df.groupby('linked_flag').apply(lambda x: x[field].iloc[-1])\n",
    "    \n",
    "for field in sum_fields:\n",
    "    # Save original primary trip info in a new column appened with \"_original\"\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    # Replace the primary trip fields with summed data\n",
    "    primary_trips_df.loc[:,field] = sums[field]\n",
    "\n",
    "for field in max_fields:\n",
    "    # Save original primary trip info in a new column appened with \"_original\"\n",
    "    #primary_trips_df.loc[:,field + '_original'] = primary_trips_df[field]\n",
    "    # Replace the primary trip fields with summed data\n",
    "    primary_trips_df.loc[:,field] = maxes[field]\n",
    "\n",
    "#df_min_stop_time = df_stop_times.sort('stop_sequence', ascending=True).groupby('trip_id', as_index=False).first()\n",
    "##need min stop time\n",
    "#df_trips = pd.merge(left = df_trips, right = df_min_stop_time, on=['trip_id'])\n",
    "\n",
    "## Save transitline data into primary trip record\n",
    "#tr1 = pd.DataFrame(df.groupby('linked_flag')[['transitline1']].agg(lambda x: x.tolist()))\n",
    "## Create new column to store unique transitline trips\n",
    "#for each in ['transitline' + str(x) for x in xrange(1,5)]:\n",
    "#    primary_trips_df[each + '_list'] = \"\"\n",
    "#tr2 = pd.DataFrame(df.groupby('linked_flag')['transitline2'].agg(lambda x: x.tolist()))\n",
    "\n",
    "# this returns greater than zero values for a single row - a single list of a list\n",
    "\n",
    "# Collect all transitline1 values for a set in a single array\n",
    "tr1 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_1']].agg(lambda x: x.tolist()))\n",
    "tr2 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_2']].agg(lambda x: x.tolist()))\n",
    "tr3 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_3']].agg(lambda x: x.tolist()))\n",
    "tr4 = pd.DataFrame(df.groupby('linked_flag')[['transit_line_4']].agg(lambda x: x.tolist()))\n",
    "ts1 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_1']].agg(lambda x: x.tolist()))\n",
    "ts2 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_2']].agg(lambda x: x.tolist()))\n",
    "ts3 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_3']].agg(lambda x: x.tolist()))\n",
    "ts4 = pd.DataFrame(df.groupby('linked_flag')[['transit_system_4']].agg(lambda x: x.tolist()))\n",
    "\n",
    "# Add together all the transitline values (1 through 4)\n",
    "combined_transitlines = pd.DataFrame(tr1['transit_line_1'] + tr2['transit_line_2'] + tr3['transit_line_3'] + tr4['transit_line_4'])\n",
    "combined_transitsys = pd.DataFrame(ts1['transit_system_1'] + ts2['transit_system_2'] + ts3['transit_system_3'] + ts4['transit_system_4'])\n",
    "#combined_transitlines[0].iloc[0]\n",
    "\n",
    "combined_transitlines[\"tr1\"] = \"\"\n",
    "combined_transitlines[\"tr2\"] = \"\"\n",
    "combined_transitlines[\"tr3\"] = \"\"\n",
    "combined_transitlines[\"tr4\"] = \"\"\n",
    "combined_transitsys[\"ts1\"] = \"\"\n",
    "combined_transitsys[\"ts2\"] = \"\"\n",
    "combined_transitsys[\"ts3\"] = \"\"\n",
    "combined_transitsys[\"ts4\"] = \"\"\n",
    "\n",
    "# Number of columns for transit lines or transit systems (4 in 2014 survey design)\n",
    "num_transitlines = 4\n",
    "num_transys = 4\n",
    "\n",
    "for row in xrange(0, len(combined_transitlines)):\n",
    "    # Add all unlinked trips' transitline data into a list\n",
    "    combined_transitlines[0].iloc[row] = unique_ordered_list(combined_transitlines[0].iloc[row])  #[0] selects df column\n",
    "    combined_transitsys[0].iloc[row] = unique_ordered_list(combined_transitsys[0].iloc[row])  #[0] selects df column\n",
    "    # Remove zeros that might be at beginning of the list\n",
    "    combined_transitlines[0].iloc[row] = [x for x in combined_transitlines[0].iloc[row] if x != 0]\n",
    "    combined_transitsys[0].iloc[row] = [x for x in combined_transitsys[0].iloc[row] if x != 0]\n",
    "    # But we want to pad the rest with zeros for consistent array shape\n",
    "    combined_transitlines[0].iloc[row] = np.pad(combined_transitlines[0].iloc[row],\n",
    "                                                (0,num_transitlines-len(combined_transitlines[0].iloc[row])),\n",
    "                                                mode='constant')\n",
    "    combined_transitsys[0].iloc[row] = np.pad(combined_transitsys[0].iloc[row],\n",
    "                                                (0,num_transitlines-len(combined_transitsys[0].iloc[row])),\n",
    "                                                mode='constant')\n",
    "\n",
    "    for i in xrange(4):\n",
    "        combined_transitlines[\"tr\" + str(i + 1)].iloc[row] = combined_transitlines[0].iloc[row][i]\n",
    "        combined_transitsys[\"ts\" + str(i + 1)].iloc[row] = combined_transitsys[0].iloc[row][i]\n",
    "\n",
    "# Add the transitline values to the primary trip record\n",
    "for i in xrange(1,5):\n",
    "    primary_trips_df['transit_line_' + str(i)] = combined_transitlines['tr' + str(i)]\n",
    "    primary_trips_df['transit_system_' + str(i)] = combined_transitsys['ts' + str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trips with all unlinked trips removed\n",
    "# note the \"-trip\" call to grab inverse of selection, so we're getting all survey trips NOT in unlinked_trips_df\n",
    "trip_unlinked_removed_all = trip[-trip['tripid'].isin(unlinked_trips_df.tripid)]   # ALL unlinked trips removed\n",
    "\n",
    "###########\n",
    "# Manually process list of trips to be linked optionally\n",
    "\n",
    "# # Okay now we want to filter out some bad linked trips and just import the unlinked trip\n",
    "# # Do this before we add the linked trips onto the main file\n",
    "# home2home = primary_trips_df.query(\"place_end == 'HOME' and place_start == 'HOME'\")\n",
    "\n",
    "# # List of bad links in the manually ID'ed linked_flag value of trips that look incorrect.\n",
    "# # Remove these from the auto-linked trip and keep in the unlinked trip file\\\n",
    "# #bad_links = pd.read_csv(bad_trips)\n",
    "# with open(bad_trips, 'r') as f:\n",
    "#     bad_trip_list = []\n",
    "#     for item in f:\n",
    "#         bad_trip_list.append(item[:-1])\n",
    "\n",
    "# bad_trip_df = primary_trips_df[primary_trips_df['linked_flag'].isin(bad_trip_list)]\n",
    "# # Append the home2home trips on the bad_trip_df\n",
    "# bad_trip_df = bad_trip_df.append(home2home)\n",
    "\n",
    "# # Remove bad trips from combined trip file\n",
    "# primary_trips_df = primary_trips_df[-primary_trips_df['tripID'].isin(bad_trip_df.tripID)]\n",
    "# primary_trips_df['linked_flag'] = primary_trips_df.index\n",
    "\n",
    "# # Add unlinked trips back in to unlinked file\n",
    "# #unlinked_trips_df = unlinked_trips_df.append(unlinked_trips_df[unlinked_trips_df['linked_flag'].isin(bad_trip_df.linked_flag)])     \n",
    "############\n",
    "\n",
    "# Trips with all linked trips added (and unlinked trips removed)\n",
    "trip_with_linked = pd.concat([trip_unlinked_removed_all,primary_trips_df])\n",
    "\n",
    "# List of still unlinked trips - these still need to be addressed\n",
    "unprocessed_unlinked_trips = unlinked_trips_df[unlinked_trips_df['combined_modes'].str.count('-') >= trip_set_max]\n",
    "# unprocessed_unlinked_trips = unprocessed_unlinked_trips.append(unlinked_trips_df[unlinked_trips_df['linked_flag'].isin(bad_trip_df.linked_flag)])    \n",
    "\n",
    "# # Distribution of combined trip modes\n",
    "# a = primary_trips_df.groupby('combined_modes').count()['recordID']\n",
    "\n",
    "# # Add the count of unlinked trips in each linked trip \n",
    "trip_with_linked['num_trips_linked'] = df_setsize[1]\n",
    "trip_with_linked['num_trips_linked'].fillna(0)\n",
    "\n",
    "# Reorder columns to match original trip file\n",
    "# trip_with_linked.columns(trip_unlinked_removed_all.columns)\n",
    "\n",
    "# Send to excel\n",
    "writer = pd.ExcelWriter(r'J:\\Projects\\Surveys\\HHTravel\\Survey2017\\Data\\Dataset_2 August 2017\\Trips\\Linked\\5-Trip-rMove-v4-LINKED.xlsx')\n",
    "\n",
    "# Trip file with ALL unlinked files removed and new linked trips added (reording cols to match original trip file order)\n",
    "# trip_with_linked.to_excel(writer, \"Linked Trips Combined\", cols=list(trip_unlinked_removed_all.columns) + ['combined_modes', 'num_trips_linked'])\n",
    "trip_with_linked.to_excel(writer, \"Linked Trips Combined\")\n",
    "\n",
    "# Trips with ALL unlinked trips removed\n",
    "trip_unlinked_removed_all.to_excel(writer, 'All Unlinked Trips Removed')\n",
    "\n",
    "# Linked Trips only\n",
    "# Join with regular trip file data\n",
    "primary_trips_df.to_excel(writer, 'Linked Trips Only')\n",
    "\n",
    "# Unlinked Trips only\n",
    "unlinked_trips_df.to_excel(writer, 'Unlinked Trips Only')\n",
    "\n",
    "# List of unprocessed unlinked trips\n",
    "unprocessed_unlinked_trips.to_excel(writer, \"Unprocessed Unlinked Trips\")\n",
    "\n",
    "# Unlinked trips that need to be edited by hand\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
